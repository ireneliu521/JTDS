---
title: 'The Twump Market'
output:
  slidy_presentation: default
---  
```{r NetworkPics2004conf, fig.cap='', fig.align='center', out.width='75%', fig.asp=0.8, echo=FALSE}
knitr::include_graphics("DSTMAA_images/NetworkPics2004conf.png")
```
#Finance data
```{r}

library(quantmod) 
SP500 = getSymbols(na.omit("^GSPC",from = as.Date("2016-01-01")))
DJIA = getSymbols(na.omit("^DJI",from = as.Date("2016-01-01")))
Nasdaq = getSymbols(na.omit("^IXIC",from = as.Date("2016-01-01")))
DRSP500= dailyReturn(GSPC)
GSPC_Ret = merge(GSPC,DRSP500)
DRDJI= dailyReturn(DJI)
DJI_Ret = merge(DJI,DRDJI)
DRNDQ= dailyReturn(IXIC)
IXIC_Ret = merge(IXIC,DRNDQ)

```
#The Tweets
```{r}
tweets= read.csv("DSTMAA_data/Crowdbabble_Social-Media-Analytics_Twitter-Download_Donald-Trump_7375-Tweets.csv")
tweets= tweets[,c(1:3,9,10)]
library(lubridate)
tweets$Cdate = ymd(tweets$Date)
tweets2017= read.csv("DSTMAA_data/trump_tweets_2017.csv")
tweets2017 = tweets2017[,1:5]
names(tweets2017)=c("Date","Time","Tweet_Text","twt_favourites_IS_THIS_LIKE_QUESTION_MARK","Retweets")
tweets2017$Cdate = mdy(tweets2017$Date)
tweets=rbind(tweets,tweets2017)
```
#building a sector dictionary
```{r}
#sector dictionary
library(jsonlite)
library(plyr)
word=c("health&care","transportation","finance","technology","capital&goods","consumer&services","consumer&non-durables","public&utilities","miscellaneous","basic&industries","energy")
url= paste("https://api.datamuse.com/words?rel_trg=",word,"&max=200",sep="")
dict=data.frame()

for(i in 1:length(url)){
  temp =fromJSON(url[i])
  dict[1:nrow(temp),i]=temp[1]
}
colnames(dict)=word
```

#Mood scores and Sector split
```{r}

#mood dictionary 
HIDict = readLines("DSTMAA_data/inqdict.txt")
dict_pos = HIDict[grep("Pos",HIDict)]
poswords = NULL
for (s in dict_pos) {
    s = strsplit(s,"#")[[1]][1]
    poswords = c(poswords,strsplit(s," ")[[1]][1])
}
dict_neg = HIDict[grep("Neg",HIDict)]
negwords = NULL
for (s in dict_neg) {
    s = strsplit(s,"#")[[1]][1]
    negwords = c(negwords,strsplit(s," ")[[1]][1])
}
poswords = tolower(poswords)
negwords = tolower(negwords)
#mood score+sector split
library(tm)
library(stringr)
mood=matrix(,length(tweets$Tweet_Text),2)
colnames(mood)=c("positive_score","negative_score")
sectormatrix=matrix(,length(tweets$Tweet_Text),11)
colnames(sectormatrix)=word
for(i in 1:length(tweets$Tweet_Text)){
    text = as.character(tweets$Tweet_Text[i])
    text = unlist(strsplit(text," "))
    ctext = Corpus(VectorSource(text))
    ctext = tm_map(ctext, removeWords, stopwords("english"))
    ctext = tm_map(ctext, removePunctuation) 
    ctext = tm_map(ctext, tolower)
    text = NULL
        for (j in 1:length(ctext)) {
            temp = ctext[[j]]$content
            if (temp!="") { text = c(text,temp) }
        }
        text = as.array(text)
        text = paste(text,collapse="\n")
        text = str_replace_all(text, "[\r\n]" , " ")
        text = unlist(strsplit(text," "))
posmatch = match(text,poswords)
numposmatch = length(posmatch[which(posmatch>0)])
negmatch = match(text,negwords)
numnegmatch = length(negmatch[which(negmatch>0)])
mood[i,]=c(numposmatch,numnegmatch)

for(j in 1:11){
sectormatch = match(text,as.character(dict[,j]))
sectorcount=length(sectormatch[which(sectormatch>0)])
sectormatrix[i,j]=sectorcount
}
}
mood=as.data.frame(mood)
mood$total_score=mood$positive_score-mood$negative_score
tweets=cbind(tweets,mood)
sectormatrix2=as.data.frame(sectormatrix)
sectormatrix2$num_industies=rowSums(sectormatrix2)
tweets=cbind(tweets,sectormatrix2)
```
#the master dataset
```{r}
#data$Cyear = year(data$Cdate)
#fd = aggregate(count~Cyear,data,sum)
#tw=aggregate() 

#retweet=aggregate(Retweets~Date,tweets,sum)
#Moodscore = aggregate(tweets[,15]~Date,tweets,sum)
#fav= aggregate(tweets[,9]~Date,tweets,sum)
#tweets1=merge(retweet,Moodscore, by = "Date")
#tweets1=merge(tweets1,fav, by = "Date")

#names(tweets1)=c("Date","Retweets","Mood Score","Favs")

masterdata1=aggregate(tweets[,4]~Cdate,tweets,sum)
masterdata1=cbind(masterdata1,aggregate(tweets[,5]~Cdate,tweets,sum)[,2])
for(i in 7:21){
test=aggregate(tweets[,i]~Cdate,tweets,sum)
masterdata1=cbind(masterdata1,test[,2])
}
colnames(masterdata1)=c("Cdate" ,"Favorite","Retweets",colnames(tweets)[7:21])

library(lubridate)
GSPC_Ret= data.frame(date=index(GSPC_Ret), coredata(GSPC_Ret))
GSPC_Ret$Cdate = ymd(GSPC_Ret$date)
GSPC_Ret$Jump<-ifelse(GSPC_Ret[,8]>0,1,0)
DJI_Ret= data.frame(date=index(DJI_Ret), coredata(DJI_Ret))
DJI_Ret$Cdate = ymd(DJI_Ret$date)
DJI_Ret$Jump<-ifelse(DJI_Ret[,8]>0,1,0)
IXIC_Ret= data.frame(date=index(IXIC_Ret), coredata(IXIC_Ret))
IXIC_Ret$Cdate = ymd(IXIC_Ret$date)
IXIC_Ret$Jump<-ifelse(IXIC_Ret[,8]>0,1,0)

masterdata = merge(masterdata1,GSPC_Ret, by ="Cdate")
masterdata = merge(masterdata,IXIC_Ret, by ="Cdate")
masterdata = merge(masterdata,DJI_Ret, by ="Cdate")
masterdata$Favorite = as.numeric(masterdata$Favorite)
masterdata$Retweets = as.numeric(masterdata$Retweets)

#masterdata$Jump<-ifelse(masterdata$daily.returns>0,1,0)
#masterdata$Jump = as.numeric(masterdata$Jump)
idx= complete.cases(masterdata)
masterdata= masterdata[idx,]

```
#plot
```{r}
par(mfrow=c(2,1))
plot(masterdata$Cdate,masterdata[,6], "l",col="red", ylab = "Mood Score",xlab = "Date")
plot(masterdata$Cdate,masterdata[,26]*1000,"l",col = "blue",ylab = "SP500 Return",xlab = "Date")

```
#SVM
```{r}
x=masterdata[,c(2,3,6:17)]
y=masterdata$Jump.x
xtrain=masterdata[1:200,c(2,3,6:17)]
ytrain=masterdata$Jump.x[1:200]
xtest=masterdata[201:469,c(2,3,6:17)]
ytest=masterdata$Jump.x[201:469]
library(e1071)
#on all data
model<-svm(x,y)
model
out<-predict(model,x)
fitted<-round(out,0)
fitted<-matrix(fitted)
table=table(y,fitted)
table
return_prediction<-ifelse(out >= 0.5,1,0)
misClasificError<-mean(return_prediction != y)
print(paste('Accuracy',1-misClasificError))
chisq.test(table)

#split to train and test
model<-svm(xtrain,ytrain)
model
out<-predict(model,xtrain)
fitted<-round(out,0)
fitted<-matrix(fitted)
table=table(ytrain,fitted)
table
return_prediction<-ifelse(out >= 0.5,1,0)
misClasificError<-mean(return_prediction != ytrain)
print(paste('Accuracy',1-misClasificError))
chisq.test(table)


out<-predict(model,xtest)
fitted<-round(out,0)
fitted<-matrix(fitted)
table=table(ytest,fitted)
table
return_prediction<-ifelse(out >= 0.5,1,0)
misClasificError<-mean(return_prediction != ytest)
print(paste('Accuracy',1-misClasificError))
chisq.test(table)
```
#wordcloud
```{r}
library(wordcloud)
library(data.table)
#pre
tweets=data.table(tweets,key="Cdate")
cloudtextpre=tweets$Tweet_Text[1:7730]
cloudtextpre=Corpus(VectorSource(cloudtextpre))
cloudtextpre = tm_map(cloudtextpre, removeWords, stopwords("english"))
cloudtextpre = tm_map(cloudtextpre, removePunctuation) 
tdm = TermDocumentMatrix(cloudtextpre,control=list(minWordLength=1))
tdmmatrix=as.matrix(tdm)
mostcommon = sort(rowSums(tdmmatrix), decreasing=TRUE)
hundredwords=rownames(as.matrix(mostcommon[1:100]))
wordcloud(hundredwords,mostcommon)
##post 
cloudtext=tweets$Tweet_Text[7730:nrow(tweets)]
cloudtext=Corpus(VectorSource(cloudtext))
cloudtext = tm_map(cloudtext, removeWords, stopwords("english"))
cloudtext = tm_map(cloudtext, removePunctuation) 
tdm = TermDocumentMatrix(cloudtext,control=list(minWordLength=1))
tdmmatrix=as.matrix(tdm)
mostcommon = sort(rowSums(tdmmatrix), decreasing=TRUE)
hundredwords=rownames(as.matrix(mostcommon[1:100]))
wordcloud(hundredwords,mostcommon)
```
#topics
```{r}
library(topicmodels)
tdmtopic = TermDocumentMatrix(cloudtextpre,control=list(minWordLength=1))
dtm=t(tdmtopic)
dtm = dtm[1:500,]

res=LDA(dtm, 5, method="Gibbs", control = list(nstart = 5, seed = list(2003,5,63,100001,765), best = TRUE, burnin = 4000, iter = 2000, thin = 500))
res.terms = as.matrix(terms(res,25))
print(res.terms)



tdmtopic2 = TermDocumentMatrix(cloudtext,control=list(minWordLength=1))
dtm=t(tdmtopic2)
dtm = dtm[1:500,]

res=LDA(dtm, 5, method="Gibbs", control = list(nstart = 5, seed = list(2003,5,63,100001,765), best = TRUE, burnin = 4000, iter = 2000, thin = 500))
res.terms = as.matrix(terms(res,25))
print(res.terms)
```
#mood
```{r}
par(mfrow=c(1,1))
plot(masterdata$Cdate,masterdata[,6], "l",col="red", ylab = "Mood Score",xlab = "Date")
abline(lm(masterdata[,6]~ masterdata$Cdate ))


sum(masterdata[1:376,6])/376
sum(masterdata[377:469,6])/(469-377+1)


library(rbokeh)
tweets$Count=rep(1,1,nrow(tweets))
tweets$Day = wday(tweets$Cdate)
tweetsag = aggregate(Count~Cdate,tweets,sum)
names(tweetsag)= c("Cdate","Count")
tweets$Count = 1
figure(width=900,height=900) %>% ly_points(x="Cdate",y="Count",data=tweetsag,hover=c(Cdate,Count))%>% ly_lines(x="Cdate",y="Count",data=tweetsag)
 
#by day of week total, we need to split this for pre and post president
library(ggplot2)
ggplot(tweets,  aes(Day)) + geom_bar() +labs(title = "Daily Tweet Count", y = "Tweet Count", x = "Day")


figure(width=900,height=900) %>% ly_points(x="Cdate",y="Count",data=tweetsag,hover=c(Cdate,Count))%>% ly_lines(x="Cdate",y="Count",data=tweetsag)

figure(width=900,height=900) %>% ly_points(x="Cdate",y="total_score",data=masterdata,hover=c(Cdate,total_score))%>% ly_lines(x="Cdate",y="total_score",data=masterdata,hover=c(Cdate,total_score))

figure(width=900,height=900) %>% ly_points(x="Cdate",y="total_score",data=masterdata,hover=c(Cdate,total_score))%>% ly_lines(x="Cdate",y="total_score",data=masterdata,hover=c(Cdate,total_score))%>% ly_lines(x="Cdate",y="Count",data=tweetsag,col = "red")

```
Lowest mood- was colorado trump rally

#avg tweets per day of week pre and post presidency

```{r}
idx = tweets$Cdate > "2017-01-19"
tweetspost = tweets[idx,]
idx = tweets$Cdate < "2017-01-20"
tweetspre = tweets[idx,]

tweetspresum = aggregate(tweetspre$Count~Cdate+Day,tweetspre,sum)
names(tweetspresum)= c("Cdate","Day","Count")
tweetspreavg = aggregate(Count~Day,tweetspresum,mean)
tweetspostsum = aggregate(tweetspost$Count~Cdate+Day,tweetspost,sum)
names(tweetspostsum)= c("Cdate","Day","Count")
tweetspostavg = aggregate(Count~Day,tweetspostsum,mean)

TweetsPrevsPostAvg =merge(tweetspreavg,tweetspostavg, by = "Day")
names(TweetsPrevsPostAvg) = c("Day","PrePresAvg","PostPresAvg")
TweetsPrevsPostAvg

```
#correlation
```{r}
x = as.matrix(tweets[,9:21])
y= as.matrix(tweets[,4])
res =lm(y~x)
summary(res)

x = as.matrix(tweets[,9:21])
y= as.matrix(tweets[,5])
res =lm(y~x)
summary(res)

x = as.matrix(tweetspre[,9:21])
y= as.matrix(tweetspre[,4])
res =lm(y~x)
summary(res)

x = as.matrix(tweetspre[,9:21])
y= as.matrix(tweetspre[,5])
res =lm(y~x)
summary(res)

x = as.matrix(tweetspost[,9:21])
y= as.matrix(tweetspost[,4])
res =lm(y~x)
summary(res)

x = as.matrix(tweetspost[,9:21])
y= as.matrix(tweetspost[,5])
res =lm(y~x)
summary(res)

```
Also kind of interesting behavior that before his presidency the # of favs and retweets were correlated to his mood score of tweets, after presidency favs and RTs are not statistically significantly correlated. 
#text summary
```{r}
text_summary = function(text, n) {
  m = length(text)  # No of sentences in input
  jaccard = matrix(0,m,m)  #Store match index
  for (i in 1:m) {
    for (j in i:m) {
      a = text[i]; aa = unlist(strsplit(a," "))
      b = text[j]; bb = unlist(strsplit(b," "))
      jaccard[i,j] = length(intersect(aa,bb))/
                          length(union(aa,bb))
      jaccard[j,i] = jaccard[i,j]
    }
  }
  similarity_score = rowSums(jaccard)
  res = sort(similarity_score, index.return=TRUE,
          decreasing=TRUE)
  idx = res$ix[1:n]
  summary = text[idx]
}

presum1=text_summary(as.character(tweets$Tweet_Text[1:1000]),10)
presum2=text_summary(as.character(tweets$Tweet_Text[1001:2000]),10)
presum3=text_summary(as.character(tweets$Tweet_Text[2001:3000]),10)
presum4=text_summary(as.character(tweets$Tweet_Text[3001:4000]),10)
presum5=text_summary(as.character(tweets$Tweet_Text[4001:5000]),10)
presum6=text_summary(as.character(tweets$Tweet_Text[5001:6000]),10)
presum7=text_summary(as.character(tweets$Tweet_Text[6001:7000]),10)
presum8=text_summary(as.character(tweets$Tweet_Text[7001:7730]),10)
sumslist=list(presum1,presum2,presum3,presum4,presum5,presum6,presum7,presum8)
sumslist=unlist(sumslist)
presum=text_summary(as.character(sumslist),10)
presum
postsum=text_summary(as.character(tweets$Tweet_Text[7731:8413]),10)
postsum
```




